{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "898/898 - 236s - loss: 1.8818 - accuracy: 0.2325 - val_loss: 1.7566 - val_accuracy: 0.2901\n",
      "Epoch 2/30\n",
      "898/898 - 232s - loss: 1.6790 - accuracy: 0.3305 - val_loss: 1.4838 - val_accuracy: 0.4166\n",
      "Epoch 3/30\n",
      "898/898 - 234s - loss: 1.5209 - accuracy: 0.4152 - val_loss: 1.3992 - val_accuracy: 0.4609\n",
      "Epoch 4/30\n",
      "898/898 - 258s - loss: 1.4317 - accuracy: 0.4561 - val_loss: 1.3186 - val_accuracy: 0.5001\n",
      "Epoch 5/30\n",
      "898/898 - 251s - loss: 1.3739 - accuracy: 0.4799 - val_loss: 1.2789 - val_accuracy: 0.5266\n",
      "Epoch 6/30\n",
      "898/898 - 250s - loss: 1.3366 - accuracy: 0.4974 - val_loss: 1.2560 - val_accuracy: 0.5233\n",
      "Epoch 7/30\n",
      "898/898 - 250s - loss: 1.3141 - accuracy: 0.5068 - val_loss: 1.2360 - val_accuracy: 0.5450\n",
      "Epoch 8/30\n",
      "898/898 - 250s - loss: 1.2851 - accuracy: 0.5184 - val_loss: 1.2419 - val_accuracy: 0.5277\n",
      "Epoch 9/30\n",
      "898/898 - 244s - loss: 1.2531 - accuracy: 0.5316 - val_loss: 1.1777 - val_accuracy: 0.5634\n",
      "Epoch 10/30\n",
      "898/898 - 231s - loss: 1.2317 - accuracy: 0.5414 - val_loss: 1.1841 - val_accuracy: 0.5531\n",
      "Epoch 11/30\n",
      "898/898 - 231s - loss: 1.2061 - accuracy: 0.5520 - val_loss: 1.1842 - val_accuracy: 0.5603\n",
      "Epoch 12/30\n",
      "898/898 - 231s - loss: 1.1881 - accuracy: 0.5617 - val_loss: 1.1320 - val_accuracy: 0.5695\n",
      "Epoch 13/30\n",
      "898/898 - 233s - loss: 1.1700 - accuracy: 0.5646 - val_loss: 1.1457 - val_accuracy: 0.5731\n",
      "Epoch 14/30\n",
      "898/898 - 231s - loss: 1.1468 - accuracy: 0.5791 - val_loss: 1.1660 - val_accuracy: 0.5704\n",
      "Epoch 15/30\n",
      "898/898 - 231s - loss: 1.1292 - accuracy: 0.5861 - val_loss: 1.1175 - val_accuracy: 0.5915\n",
      "Epoch 16/30\n",
      "898/898 - 238s - loss: 1.1106 - accuracy: 0.5902 - val_loss: 1.0994 - val_accuracy: 0.5918\n",
      "Epoch 17/30\n",
      "898/898 - 236s - loss: 1.0990 - accuracy: 0.5980 - val_loss: 1.1254 - val_accuracy: 0.5782\n",
      "Epoch 18/30\n",
      "898/898 - 233s - loss: 1.0746 - accuracy: 0.6052 - val_loss: 1.1220 - val_accuracy: 0.5882\n",
      "Epoch 19/30\n",
      "898/898 - 232s - loss: 1.0694 - accuracy: 0.6078 - val_loss: 1.0922 - val_accuracy: 0.5832\n",
      "Epoch 20/30\n",
      "898/898 - 231s - loss: 1.0508 - accuracy: 0.6122 - val_loss: 1.0884 - val_accuracy: 0.5935\n",
      "Epoch 21/30\n",
      "898/898 - 232s - loss: 1.0389 - accuracy: 0.6205 - val_loss: 1.0688 - val_accuracy: 0.6010\n",
      "Epoch 22/30\n",
      "898/898 - 231s - loss: 1.0181 - accuracy: 0.6288 - val_loss: 1.0511 - val_accuracy: 0.6063\n",
      "Epoch 23/30\n",
      "898/898 - 232s - loss: 1.0100 - accuracy: 0.6303 - val_loss: 1.0746 - val_accuracy: 0.6007\n",
      "Epoch 24/30\n",
      "898/898 - 232s - loss: 0.9995 - accuracy: 0.6316 - val_loss: 1.0494 - val_accuracy: 0.6082\n",
      "Epoch 25/30\n",
      "898/898 - 232s - loss: 0.9943 - accuracy: 0.6339 - val_loss: 1.0525 - val_accuracy: 0.6135\n",
      "Epoch 26/30\n",
      "898/898 - 231s - loss: 0.9818 - accuracy: 0.6437 - val_loss: 1.0597 - val_accuracy: 0.6069\n",
      "Epoch 27/30\n",
      "898/898 - 231s - loss: 0.9674 - accuracy: 0.6475 - val_loss: 1.0318 - val_accuracy: 0.6177\n",
      "Epoch 28/30\n",
      "898/898 - 231s - loss: 0.9563 - accuracy: 0.6509 - val_loss: 1.0577 - val_accuracy: 0.6141\n",
      "Epoch 29/30\n",
      "898/898 - 232s - loss: 0.9432 - accuracy: 0.6558 - val_loss: 1.0271 - val_accuracy: 0.6163\n",
      "Epoch 30/30\n",
      "898/898 - 231s - loss: 0.9356 - accuracy: 0.6597 - val_loss: 1.0383 - val_accuracy: 0.6152\n"
     ]
    }
   ],
   "source": [
    "# Model with 61% Accuracy !\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Dropout,Activation,Flatten,Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# I previosuly explored the dataset file : Dataset_overview \n",
    "\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "\n",
    "# Split the data into train and test\n",
    "\n",
    "X_train, train_y , X_test , test_y = [],[],[],[]\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    pixls = row['pixels'].split(\" \") # Pixels are separated by a space, we store them as a list called pixls\n",
    "    try:\n",
    "        if 'Training' in row ['Usage']: # Assign values to train sets\n",
    "            X_train.append(np.array(pixls,'float32')) # pixels converted to float numpy arrays (needed for keras and normalization)\n",
    "            train_y.append(row['emotion']) # our target\n",
    "        elif 'PublicTest' in row['Usage']: # Assign values to test sets\n",
    "            test_y.append(row['emotion']) \n",
    "            X_test.append(np.array(pixls,'float32'))  \n",
    "    except: \n",
    "        print('Error found: index {index} row :{row}')        \n",
    "\n",
    "\n",
    "# Convert test and train sets into numpy arrays (needed for keras and normalization)\n",
    "\n",
    "X_train= np.array( X_train ,'float32')\n",
    "X_test= np.array( X_test ,'float32')\n",
    "train_y= np.array( train_y ,'float32')\n",
    "test_y= np.array( test_y ,'float32')\n",
    "\n",
    "# Normalization: substract the mean and divide it by the standard deviation (to convert all from 0 to 1 values)\n",
    "\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)  \n",
    "\n",
    "# Parameters to use later\n",
    "\n",
    "num_features=64 \n",
    "num_labels=7 # the emotions that we want to predict\n",
    "batch_size=32 # Number of samples processed before the model is updated\n",
    "epochs=30 # Number of complete passes through the training dataset\n",
    "width,height = 48,48  # to reshape image size\n",
    "\n",
    "# Reshape \"X\" for keras using the width,height = 48,48\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],width,height,1) # 0 is for the row, 1 means one image will have this widht and height\n",
    "X_test = X_test.reshape(X_test.shape[0],width,height,1)\n",
    "\n",
    "\n",
    "# Change \"Y\" to categorical (in order to use categorical_crossentropy later)\n",
    "# It converts an array into a matrix : we'll have as many colums as they are classes kinda like dummies in numpy ( the rows stay the same)\n",
    "\n",
    "train_y=np_utils.to_categorical(train_y,num_classes=num_labels)\n",
    "test_y=np_utils.to_categorical(test_y,num_classes=num_labels)\n",
    "\n",
    "# Choose model \n",
    "\n",
    "model= Sequential() # a linear stack of layers ( no shared layers or multiple inputs or outputs)\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "# In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer.\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),input_shape=(48,48,1),activation='relu')) # shape not for all the row just (width,height,1)\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) # Downsamples the input representation\n",
    "model.add(Dropout(0.5))   # Instead of Dropout to standardize the outputs of a hidden layer\n",
    "\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())# 128 filters\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "# In the next layer I had to add padding to avoid the error message\n",
    "# Same padding applies padding to the input image to get it fully covered by the filter and specified stride.\n",
    "# It is called same because, for stride 1 , the output will be the same as the input.\n",
    "\n",
    "model.add(Conv2D(256,(3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "# Flattening and adding two fully connected layers (to make a big vector for dense layers)\n",
    "\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Add two fully connected layers \n",
    "# In a dense layer (fully connected layer), each neuron receives input from every element of the previous layer. \n",
    "\n",
    "model.add(Dense(1024,activation='relu'))  # 1024 filters\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(Dense(1024,activation='relu'))  # 1024 filters\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final layer : is softclas because we're doing multiclassification (7 labels / emotions)\n",
    "\n",
    "model.add(Dense(7, activation=\"softmax\")) \n",
    "\n",
    "# Compile model: Computes the crossentropy loss between the labels and predictions\n",
    "# Crossentropy is the difference between two probability distributions\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy']) \n",
    "\n",
    "# Fit the model\n",
    "\n",
    "model.fit(X_train,train_y,batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=2,  # verbose = 1, which includes both progress bar and one line per epoch. verbose = 0, means silent. verbose = 2, one line per epoch i.e. epoch no./total no. of epochs.\n",
    "    validation_data= (X_test,test_y), # to measure accuracy with the test dataset\n",
    "    shuffle=True) # to ensure that each data point creates an \"independent\" change on the model, without being biased by the same points before them.\n",
    "\n",
    "# Save model version for the video tester\n",
    "\n",
    "emotions_json= model.to_json()\n",
    "with open (\"model_17.json\",\"w\") as json_file:\n",
    "  json_file.write(emotions_json)\n",
    "model.save_weights(\"model_17.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
