{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "898/898 - 274s - loss: 1.8585 - accuracy: 0.2462 - val_loss: 1.6427 - val_accuracy: 0.3475\n",
      "Epoch 2/30\n",
      "898/898 - 261s - loss: 1.6318 - accuracy: 0.3579 - val_loss: 1.4863 - val_accuracy: 0.4205\n",
      "Epoch 3/30\n",
      "898/898 - 275s - loss: 1.4949 - accuracy: 0.4247 - val_loss: 1.3469 - val_accuracy: 0.4932\n",
      "Epoch 4/30\n",
      "898/898 - 231s - loss: 1.4235 - accuracy: 0.4616 - val_loss: 1.3124 - val_accuracy: 0.4923\n",
      "Epoch 5/30\n",
      "898/898 - 234s - loss: 1.3757 - accuracy: 0.4815 - val_loss: 1.2664 - val_accuracy: 0.5185\n",
      "Epoch 6/30\n",
      "898/898 - 234s - loss: 1.3379 - accuracy: 0.4930 - val_loss: 1.2613 - val_accuracy: 0.5222\n",
      "Epoch 7/30\n",
      "898/898 - 232s - loss: 1.3072 - accuracy: 0.5104 - val_loss: 1.2240 - val_accuracy: 0.5339\n",
      "Epoch 8/30\n",
      "898/898 - 230s - loss: 1.2772 - accuracy: 0.5223 - val_loss: 1.2276 - val_accuracy: 0.5464\n",
      "Epoch 9/30\n",
      "898/898 - 230s - loss: 1.2575 - accuracy: 0.5313 - val_loss: 1.1772 - val_accuracy: 0.5612\n",
      "Epoch 10/30\n",
      "898/898 - 229s - loss: 1.2382 - accuracy: 0.5369 - val_loss: 1.2016 - val_accuracy: 0.5514\n",
      "Epoch 11/30\n",
      "898/898 - 232s - loss: 1.2036 - accuracy: 0.5548 - val_loss: 1.1479 - val_accuracy: 0.5762\n",
      "Epoch 12/30\n",
      "898/898 - 232s - loss: 1.1910 - accuracy: 0.5588 - val_loss: 1.1683 - val_accuracy: 0.5578\n",
      "Epoch 13/30\n",
      "898/898 - 243s - loss: 1.1680 - accuracy: 0.5679 - val_loss: 1.1245 - val_accuracy: 0.5754\n",
      "Epoch 14/30\n",
      "898/898 - 278s - loss: 1.1548 - accuracy: 0.5750 - val_loss: 1.1184 - val_accuracy: 0.5840\n",
      "Epoch 15/30\n",
      "898/898 - 251s - loss: 1.1360 - accuracy: 0.5825 - val_loss: 1.1003 - val_accuracy: 0.5929\n",
      "Epoch 16/30\n",
      "898/898 - 237s - loss: 1.1120 - accuracy: 0.5895 - val_loss: 1.0979 - val_accuracy: 0.5910\n",
      "Epoch 17/30\n",
      "898/898 - 241s - loss: 1.1028 - accuracy: 0.5934 - val_loss: 1.1095 - val_accuracy: 0.5812\n",
      "Epoch 18/30\n",
      "898/898 - 238s - loss: 1.0870 - accuracy: 0.6021 - val_loss: 1.0747 - val_accuracy: 0.5993\n",
      "Epoch 19/30\n",
      "898/898 - 237s - loss: 1.0765 - accuracy: 0.6037 - val_loss: 1.0860 - val_accuracy: 0.5896\n",
      "Epoch 20/30\n",
      "898/898 - 296s - loss: 1.0627 - accuracy: 0.6111 - val_loss: 1.0726 - val_accuracy: 0.5985\n",
      "Epoch 21/30\n",
      "898/898 - 279s - loss: 1.0485 - accuracy: 0.6158 - val_loss: 1.0959 - val_accuracy: 0.5985\n",
      "Epoch 22/30\n",
      "898/898 - 257s - loss: 1.0338 - accuracy: 0.6210 - val_loss: 1.0640 - val_accuracy: 0.6007\n",
      "Epoch 23/30\n",
      "898/898 - 307s - loss: 1.0193 - accuracy: 0.6280 - val_loss: 1.0717 - val_accuracy: 0.6049\n",
      "Epoch 24/30\n",
      "898/898 - 308s - loss: 1.0112 - accuracy: 0.6293 - val_loss: 1.0673 - val_accuracy: 0.6018\n",
      "Epoch 25/30\n",
      "898/898 - 302s - loss: 1.0022 - accuracy: 0.6342 - val_loss: 1.0437 - val_accuracy: 0.6099\n",
      "Epoch 26/30\n",
      "898/898 - 302s - loss: 0.9829 - accuracy: 0.6404 - val_loss: 1.0484 - val_accuracy: 0.6119\n",
      "Epoch 27/30\n",
      "898/898 - 303s - loss: 0.9774 - accuracy: 0.6431 - val_loss: 1.0293 - val_accuracy: 0.6160\n",
      "Epoch 28/30\n",
      "898/898 - 308s - loss: 0.9711 - accuracy: 0.6426 - val_loss: 1.0235 - val_accuracy: 0.6230\n",
      "Epoch 29/30\n",
      "898/898 - 321s - loss: 0.9563 - accuracy: 0.6487 - val_loss: 1.0589 - val_accuracy: 0.6177\n",
      "Epoch 30/30\n",
      "898/898 - 381s - loss: 0.9498 - accuracy: 0.6527 - val_loss: 1.0518 - val_accuracy: 0.6094\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# I previosuly explored the dataset file : Dataset_overview \n",
    "\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "\n",
    "# Split the data into train and test\n",
    "\n",
    "X_train, train_y , X_test , test_y = [],[],[],[]\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    pixls = row['pixels'].split(\" \") # Pixels are separated by a space, we store them as a list called pixls\n",
    "    try:\n",
    "        if 'Training' in row ['Usage']: # Assign values to train sets\n",
    "            X_train.append(np.array(pixls,'float32')) # pixels converted to float numpy arrays (needed for keras and normalization)\n",
    "            train_y.append(row['emotion']) # our target\n",
    "        elif 'PublicTest' in row['Usage']: # Assign values to test sets\n",
    "            test_y.append(row['emotion']) \n",
    "            X_test.append(np.array(pixls,'float32'))  \n",
    "    except: \n",
    "        print(f'Error found: index {index} row :{row}')        \n",
    "\n",
    "\n",
    "\n",
    "# Convert test and train sets into numpy arrays (needed for keras and normalization)\n",
    "\n",
    "X_train= np.array( X_train ,'float32')\n",
    "X_test= np.array( X_test ,'float32')\n",
    "train_y= np.array( train_y ,'float32')\n",
    "test_y= np.array( test_y ,'float32')\n",
    "\n",
    "# Normalization: substract the mean and divide it by the standard deviation (to convert all from 0 to 1 values)\n",
    "\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)  \n",
    "\n",
    "# Parameters to use later\n",
    "\n",
    "num_features=64 \n",
    "num_labels=7 # the emotions that we want to predict\n",
    "batch_size=32 # Number of samples processed before the model is updated\n",
    "epochs=30 # Number of complete passes through the training dataset\n",
    "width,height = 48,48  # to reshape image size\n",
    "\n",
    "# Reshape \"X\" for keras using the width,height = 48,48\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],width,height,1) # 0 is for the row, 1 means one image will have this widht and height\n",
    "X_test = X_test.reshape(X_test.shape[0],width,height,1)\n",
    "\n",
    "\n",
    "# Change \"Y\" to categorical (in order to use categorical_crossentropy later)\n",
    "# It converts an array into a matrix : we'll have as many colums as they are classes kinda like dummies in numpy ( the rows stay the same)\n",
    "\n",
    "train_y=np_utils.to_categorical(train_y,num_classes=num_labels)\n",
    "test_y=np_utils.to_categorical(test_y,num_classes=num_labels)\n",
    "\n",
    "# Choose model \n",
    "\n",
    "model= Sequential() # a linear stack of layers ( no shared layers or multiple inputs or outputs)\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "# In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer.\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),input_shape=(48,48,1),activation='relu')) # shape not for all the row just (width,height,1)\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) # Downsamples the input representation\n",
    "model.add(Dropout(0.5))   # Instead of Dropout to standardize the outputs of a hidden layer\n",
    "\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "# In the next layer I had to add padding to avoid the error message\n",
    "# Same padding applies padding to the input image to get it fully covered by the filter and specified stride.\n",
    "# It is called same because, for stride 1 , the output will be the same as the input.\n",
    "\n",
    "model.add(Conv2D(256,(3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) \n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "# Flattening and adding two fully connected layers (to make a big vector for dense layers)\n",
    "\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Add two fully connected layers \n",
    "# In a dense layer (fully connected layer), each neuron receives input from every element of the previous layer. \n",
    "\n",
    "model.add(Dense(1024,activation='relu'))  # 1024 filters\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(Dense(1024,activation='relu'))  # 1024 filters\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final layer : is softclas because we're doing multiclassification (7 labels / emotions)\n",
    "\n",
    "model.add(Dense(7, activation=\"softmax\")) \n",
    "\n",
    "# Compile model: Computes the crossentropy loss between the labels and predictions\n",
    "# Crossentropy is the difference between two probability distributions\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy']) \n",
    "\n",
    "# Fit the model\n",
    "\n",
    "model.fit(X_train,train_y,batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=2,  # verbose = 1, which includes both progress bar and one line per epoch. verbose = 0, means silent. verbose = 2, one line per epoch i.e. epoch no./total no. of epochs.\n",
    "    validation_data= (X_test,test_y), # to measure accuracy with the test dataset\n",
    "    shuffle=True) # to ensure that each data point creates an \"independent\" change on the model, without being biased by the same points before them.\n",
    "\n",
    "# Save model for the video tester\n",
    "\n",
    "emotions_json= model.to_json()\n",
    "with open (\"emotions17.json\",\"w\") as json_file:\n",
    "  json_file.write(emotions_json)\n",
    "model.save_weights(\"emotions17.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
